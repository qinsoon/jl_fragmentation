import Pkg
Pkg.add("PProf")

using Profile, PProf

mutable struct ListNode{N}
    next::Union{ListNode{N}, Nothing}
    data::NTuple{N, UInt8}
end

# 2. Specialized allocation function
@inline function create_node(::Val{N}) where N
    # N is now compile-time known
    ListNode{N}(nothing, ntuple(_ -> UInt8(0), Val(N)))
end

next_ptr_bytes = 8
jl_header_bytes = 8

# julia_internal.h jl_gc_sizeclasses
jl_gc_sizeclasses = [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288, 304, 336, 368, 400, 448, 496, 544, 576, 624, 672, 736, 816, 896, 1008, 1088, 1168, 1248, 1360, 1488, 1632, 1808, 2032]
n_size_classes = length(jl_gc_sizeclasses)
page_size = 16384

# Configure this to change the length of this benchmark
pages_to_allocate = 1024 * 10
# How many objects to keep alive per page
objs_to_keep = 1

const kept_lists = []

function process_size_class(sz_class_index::Int, sz::Int)
    payload_sz = sz - jl_header_bytes
    tuple_len = payload_sz - next_ptr_bytes # Account for 8-byte next pointer and julia header
    T = Val(tuple_len)
    if tuple_len <= 0
        return
    end
    objs_per_page = div(page_size, sz)  # Objects per page (16KB/page)
    objs_to_keep <= objs_per_page || error("objs_to_keep must be less than or equal to objs_per_page")
    total_objs = Int64(pages_to_allocate * objs_per_page)    # Allocate for 1024 pages

    actual_sz = sizeof(ListNode{tuple_len})

    println("Processing size class $sz")
    println("  Actual payload size: $actual_sz bytes, N: $tuple_len")
    println("  Allocating $total_objs objects")

    print_page_utilization("Before", sz_class_index)

    actual_sz + jl_header_bytes == sz || error("Actual size does not match expected size")
    # Create initial linked list
    head = create_list(total_objs, T)
    n_nodes(head) == total_objs || error("Actual number of objects does not match total_objs")

    print_page_utilization("Alloc", sz_class_index)

    # println("  Fragmenting linked list (keep 1 object alive per every $objs_per_page objects)...")
    # Select every K-th object to keep
    new_head = fragment_list(head, objs_per_page)
    # Preserve remaining objects
    global kept_lists
    push!(kept_lists, new_head)

    print_page_utilization("Fragment", sz_class_index)
end

function create_list(total_objs::Int64, T::Val)
    # We need to be careful that the loop does not introduce allocations
    head = current = create_node(T)
    i = 2
    while true
        current.next = create_node(T)
        current = current.next
        i += 1
        i > total_objs && break
    end
    return head
end

function fragment_list(head::ListNode, objs_per_page::Int)
    new_head = head
    last = head
    current = head
    counter = 1
    while !isnothing(current) && !isnothing(current.next)
        next = current.next
        if mod(counter, objs_per_page) < objs_to_keep
            last.next = current
            last = current
        end
        current = next
        counter += 1
    end
    return new_head
end

function iterate_nodes(node::ListNode, count::Int)
    for _ in 1:count
        isnothing(node.next) && break
        node = node.next
    end
    return node
end

function n_nodes(node::ListNode)
    count = 0
    current = node
    while !isnothing(current)
        count += 1
        current = current.next
    end
    return count
end

struct GCFragmentationStats
    n_freed_objs::Csize_t
    n_pages_allocd::Csize_t
end

function print_page_utilization(msg::String, sz_class_index::Int)
    full_heap_gc()

    # To get page utilization details, apply the following patch to Julia
    #     @@ -826,6 +827,7 @@ int jl_gc_classify_pools(size_t sz, int *osize)
    #
    #     gc_fragmentation_stat_t gc_page_fragmentation_stats[JL_GC_N_POOLS];
    #     JL_DLLEXPORT double jl_gc_page_utilization_stats[JL_GC_N_MAX_POOLS];
    #    +JL_DLLEXPORT gc_fragmentation_stat_t jl_gc_page_fragmentation_stats_export[JL_GC_N_MAX_POOLS];
    #
    #     STATIC_INLINE void gc_update_page_fragmentation_data(jl_gc_pagemeta_t *pg) JL_NOTSAFEPOINT
    #     {
    #    @@ -845,6 +847,8 @@ STATIC_INLINE void gc_dump_page_utilization_data(void) JL_NOTSAFEPOINT
    #                 utilization -= ((double)n_freed_objs * (double)jl_gc_sizeclasses[i]) / (double)n_pages_allocd / (double)GC_PAGE_SZ;
    #             }
    #             jl_gc_page_utilization_stats[i] = utilization;
    #    +        jl_gc_page_fragmentation_stats_export[i].n_freed_objs = stats->n_freed_objs;
    #    +        jl_gc_page_fragmentation_stats_export[i].n_pages_allocd = stats->n_pages_allocd;
    #             jl_atomic_store_relaxed(&stats->n_freed_objs, 0);
    #             jl_atomic_store_relaxed(&stats->n_pages_allocd, 0);
    #         }

    # Then get pointer to the GC page utilization array
    # ptr = cglobal(:jl_gc_page_fragmentation_stats_export, GCFragmentationStats)
    # stats = unsafe_wrap(Array, ptr, (n_size_classes,), own=false)

    utils = Base.gc_page_utilization_data()

    for (i, sz) in enumerate(jl_gc_sizeclasses)
        if sz_class_index == -1 || i == sz_class_index
            # println("    Pool $i: $(round(utils[i]*100, digits=2))%, $(stats[i].n_freed_objs) free objs, $(stats[i].n_pages_allocd) pages allocated")
            println("    $(lpad(msg, 10)): Pool $i = $(round(utils[i]*100, digits=2))%")
        end
    end
end

function full_heap_gc()
    GC.gc(true)
end

# Run fragmentation process
# Profile.Allocs.clear()
# Profile.Allocs.@profile sample_rate=0.001 begin
    for (i,sz) in enumerate(jl_gc_sizeclasses)
        # if i > 10
        #     break
        # end
        # Julia aligns up object size to 16 bytes. We only allocate if the size class is 16 bytes aligned.
        if mod(sz, 16) != 0
            continue
        end
        process_size_class(i, sz)
        full_heap_gc()
    end
end
# prof = Profile.Allocs.fetch()
# PProf.Allocs.pprof(prof; from_c=false)
# sleep(1000)

println("\nFragmentation complete.")
print_page_utilization("Final", -1)
